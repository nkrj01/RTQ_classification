{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdr5JdNKfzWJW/E5pGHyk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkrj01/Unit-ops-classification/blob/main/keras_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "In this notebook, I have created an NLP model to clasify health authority questions into certain themes which can be used to gain important insights to drive important process changes"
      ],
      "metadata": {
        "id": "PeN6wSr34q2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing libraries**"
      ],
      "metadata": {
        "id": "loZBqD0N5Q7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg2t8dVN4igu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "nltk.download('stopwords')  # Download the stop words list\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing RTQ data**"
      ],
      "metadata": {
        "id": "hxFrqAK65YiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(r'train.csv')\n",
        "df_train = data[[\"Questions\", \"target\"]]\n",
        "# train-test split\n",
        "X_train, X_CV, y_train, y_CV = train_test_split(df_train[\"text_train\"],\n",
        "                                                df_train[\"target\"],\n",
        "                                                test_size=0.1,\n",
        "                                                random_state=21,\n",
        "                                                stratify=df_train[\"target\"])"
      ],
      "metadata": {
        "id": "fUyrh0xI5gnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Keras layer to clean and vectorize the text**"
      ],
      "metadata": {
        "id": "WbvzZKro5xde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning(text):\n",
        "  text_tf = tf.convert_to_tensor(text, dtype=tf.string)\n",
        "  lowercase = tf.strings.lower(text_tf)\n",
        "  striped_nonenglish = tf.strings.regex_replace(lowercase, '[^ -~]', '')\n",
        "  stripped_links = tf.strings.regex_replace(striped_nonenglish, r\"https?://\\S+|www\\.\\S+\", \" \")\n",
        "  stripped_special_char = tf.strings.regex_replace(\n",
        "        stripped_links, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
        "        )\n",
        "  clean_text = tf.strings.strip(stripped_special_char) # strip white space\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_word_pattern = '\\\\b(' + '|'.join(stop_words) + ')\\\\b'\n",
        "  main_words = tf.strings.regex_replace(clean_text, stop_word_pattern, '')\n",
        "\n",
        "  return main_words\n",
        "\n",
        "max_features = 1000\n",
        "embedding_dim = 128\n",
        "sequence_length = 200\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=text_cleaning,\n",
        "    max_tokens=max_features,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# fitting the vectorization layer into training data\n",
        "vectorize_layer.adapt(X_train.tolist() + X_CV.tolist())\n",
        "\n",
        "# converting to tf object for model input later\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.string)\n",
        "X_CV = tf.convert_to_tensor(X_CV, dtype=tf.string)"
      ],
      "metadata": {
        "id": "XgEiIM9y6DtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network model training without tuning**"
      ],
      "metadata": {
        "id": "TIGkJv6h9lCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype=tf.string) # input layer\n",
        "x = vectorize_layer(inputs) # vectorization\n",
        "x = layers.Embedding(max_features+1, embedding_dim)(x) # embeddings\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Conv1D(50, 4, padding=\"valid\", activation=\"relu\", strides=1)(x) # conv layer to detect relations\n",
        "x = layers.GlobalMaxPooling1D()(x) # pooling to reduce dimensionality\n",
        "x = layers.Dense(32, activation=\"relu\")(x) # dense layer\n",
        "output = layers.Dense(8, activation=\"softmax\", name=\"predictions\")(x) # multi-class output layer\n",
        "\n",
        "model_nn = tf.keras.Model(inputs, output)\n",
        "\n",
        "# compile\n",
        "model_nn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.CategoricalCrossentropy(),\n",
        "                       tf.keras.metrics.F1Score()])\n",
        "# fit\n",
        "model_nn.fit(\n",
        "    X_train,\n",
        "    y_train.to_numpy().reshape(-1, 1).astype(np.float32),\n",
        "    batch_size = 200,\n",
        "    epochs = 5,\n",
        "    validation_data=(X_CV, y_CV.to_numpy().reshape(-1, 1).astype(np.float32))\n",
        ")"
      ],
      "metadata": {
        "id": "o3HOMlHi9evd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameter tuning using Keras tuner**"
      ],
      "metadata": {
        "id": "qBUkbYfpC60V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  wrap the model inside a function\n",
        "def build_model(hp):\n",
        "  inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "  x = vectorize_layer(inputs)\n",
        "  x = layers.Embedding(max_features+1, embedding_dim)(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  # optimzing kernel size and filter numbers\n",
        "  x = layers.Conv1D(\n",
        "      hp.Int(\"filters\", min_value=8, max_value=64, step=8),\n",
        "      hp.Int(\"kernel size\", min_value=2, max_value=6, step=1),\n",
        "      padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
        "  x = layers.GlobalMaxPooling1D()(x)\n",
        "  # optimizing number of units of dense layer\n",
        "  x = layers.Dense(hp.Int(\"units\", min_value=8, max_value=64, step=8),\n",
        "                   activation=\"relu\")(x)\n",
        "  output = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
        "\n",
        "  model_nn = tf.keras.Model(inputs, output)\n",
        "\n",
        "  # compile\n",
        "  model_nn.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "                hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), # optimizing alpha\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=[tf.keras.metrics.tf.keras.metrics.SparseCategoricalAccuracy(),\n",
        "                        tf.keras.metrics.F1Score()])\n",
        "\n",
        "  return model_nn\n",
        "\n",
        "# setting up the tuner\n",
        "tuner = keras_tuner.tuners.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=10)\n",
        "\n",
        "# searching optimal model\n",
        "tuner.search(\n",
        "    X_train,\n",
        "    y_train.to_numpy().reshape(-1, 1).astype(np.float32),\n",
        "    validation_data=(X_CV, y_CV.to_numpy().reshape(-1, 1).astype(np.float32)))\n",
        "\n",
        "# retrieving optimal model\n",
        "model_nn = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "I2f7L-R3C_3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}